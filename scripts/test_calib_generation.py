#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ ¡å‡†æ•°æ®ç”Ÿæˆæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯Flux.1 devæ¨¡å‹æ ¡å‡†æ•°æ®ç”Ÿæˆçš„æ­£ç¡®æ€§
"""

import os
import sys
import torch
import traceback
from pathlib import Path

# æ·»åŠ fluxæ¨¡å—åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("ğŸ” Testing model loading...")
    
    try:
        from flux.util import load_flow_model, load_t5, load_clip, load_ae
        
        # æµ‹è¯•åŠ è½½Fluxæ¨¡å‹
        print("  Loading Flux model...")
        model = load_flow_model("flux-dev", device="cuda", hf_download=True)
        print(f"  âœ“ Flux model loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B params")
        
        # æµ‹è¯•åŠ è½½T5
        print("  Loading T5 encoder...")
        t5 = load_t5(device="cuda", max_length=512)
        print("  âœ“ T5 encoder loaded")
        
        # æµ‹è¯•åŠ è½½CLIP
        print("  Loading CLIP encoder...")
        clip = load_clip(device="cuda")
        print("  âœ“ CLIP encoder loaded")
        
        # æµ‹è¯•åŠ è½½VAE
        print("  Loading VAE...")
        ae = load_ae("flux-dev", device="cuda")
        print("  âœ“ VAE loaded")
        
        return model, t5, clip, ae
        
    except Exception as e:
        print(f"  âœ— Model loading failed: {e}")
        traceback.print_exc()
        return None, None, None, None

def test_coco_data_loading():
    """æµ‹è¯•COCOæ•°æ®åŠ è½½"""
    print("\nğŸ” Testing COCO data loading...")
    
    try:
        coco_path = "/home/zcx/codes/data/coco/annotations/captions_val2014.json"
        
        if not os.path.exists(coco_path):
            print(f"  âœ— COCO annotations file not found: {coco_path}")
            return None
        
        # å¯¼å…¥å¹¶æµ‹è¯•COCOæ•°æ®åŠ è½½å‡½æ•°
        sys.path.append(os.path.dirname(__file__))
        from gen_flux_calib_data import prepare_coco_text_and_image
        
        prompts, image_paths = prepare_coco_text_and_image(coco_path, max_samples=5)
        
        print(f"  âœ“ Loaded {len(prompts)} sample prompts")
        print("  Sample prompts:")
        for i, prompt in enumerate(prompts):
            print(f"    {i+1}: {prompt[:60]}...")
            
        return prompts
        
    except Exception as e:
        print(f"  âœ— COCO data loading failed: {e}")
        traceback.print_exc()
        return None

def test_flux_inference():
    """æµ‹è¯•Fluxæ¨¡å‹æ¨ç†"""
    print("\nğŸ” Testing Flux inference...")
    
    try:
        from flux.sampling import get_noise, prepare, get_schedule
        from flux.util import load_flow_model, load_t5, load_clip
        
        # åŠ è½½æ¨¡å‹ç»„ä»¶
        model = load_flow_model("flux-dev", device="cuda")
        t5 = load_t5(device="cuda", max_length=512)
        clip = load_clip(device="cuda")
        
        # å‡†å¤‡æµ‹è¯•è¾“å…¥
        prompt = "A beautiful sunset over the ocean"
        height, width = 512, 512  # ä½¿ç”¨è¾ƒå°å°ºå¯¸è¿›è¡Œæµ‹è¯•
        
        # è·å–å™ªå£°
        img = get_noise(
            num_samples=1,
            height=height,
            width=width,
            device=torch.device("cuda"),
            dtype=torch.bfloat16,
            seed=42
        )
        
        # å‡†å¤‡è¾“å…¥
        inp = prepare(t5, clip, img, prompt)
        
        print(f"  âœ“ Input prepared:")
        print(f"    img: {inp['img'].shape}")
        print(f"    txt: {inp['txt'].shape}")
        print(f"    vec: {inp['vec'].shape}")
        print(f"    img_ids: {inp['img_ids'].shape}")
        print(f"    txt_ids: {inp['txt_ids'].shape}")
        
        # æµ‹è¯•å•æ­¥å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(
                img=inp["img"],
                img_ids=inp["img_ids"],
                txt=inp["txt"],
                txt_ids=inp["txt_ids"],
                timesteps=torch.tensor([0.5], device="cuda"),
                y=inp["vec"],
                guidance=torch.tensor([0.0], device="cuda") if model.params.guidance_embed else None,
            )
        
        print(f"  âœ“ Forward pass successful: output shape {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"  âœ— Flux inference failed: {e}")
        traceback.print_exc()
        return False

def test_calibration_data_generation():
    """æµ‹è¯•æ ¡å‡†æ•°æ®ç”Ÿæˆ"""
    print("\nğŸ” Testing calibration data generation...")
    
    try:
        sys.path.append(os.path.dirname(__file__))
        from gen_flux_calib_data import generate_flux_calibration_data, prepare_coco_text_and_image
        from flux.util import load_flow_model, load_t5, load_clip, load_ae
        
        # åŠ è½½æ¨¡å‹
        model = load_flow_model("flux-dev", device="cuda")
        t5 = load_t5(device="cuda", max_length=512)
        clip = load_clip(device="cuda")
        ae = load_ae("flux-dev", device="cuda")
        
        # å‡†å¤‡å°‘é‡æµ‹è¯•æ•°æ®
        coco_path = "/home/zcx/codes/data/coco/annotations/captions_val2014.json"
        prompts, _ = prepare_coco_text_and_image(coco_path, max_samples=2)
        
        print(f"  Testing with {len(prompts)} prompts...")
        
        # ç”Ÿæˆæ ¡å‡†æ•°æ®ï¼ˆä½¿ç”¨è¾ƒå°å°ºå¯¸å’Œè¾ƒå°‘æ­¥æ•°ï¼‰
        calibration_data = generate_flux_calibration_data(
            prompts=prompts,
            model=model,
            t5=t5,
            clip=clip,
            ae=ae,
            num_steps=2,  # è¾ƒå°‘æ­¥æ•°
            guidance=0.0,
            height=512,   # è¾ƒå°å°ºå¯¸
            width=512,
            seed=42,
            device="cuda"
        )
        
        print("  âœ“ Calibration data generated successfully!")
        print(f"  Data structure:")
        for key, value in calibration_data.items():
            if isinstance(value, torch.Tensor):
                print(f"    {key}: {value.shape} ({value.dtype})")
            elif isinstance(value, list) and key == "prompts":
                print(f"    {key}: {len(value)} prompts")
            else:
                print(f"    {key}: {value}")
        
        # éªŒè¯æ•°æ®ç»´åº¦
        assert calibration_data["xs"].shape[1] == len(prompts), "Batch size mismatch in xs"
        assert calibration_data["ts"].shape[1] == len(prompts), "Batch size mismatch in ts"
        assert len(calibration_data["prompts"]) == len(prompts), "Prompts count mismatch"
        
        print("  âœ“ Data validation passed!")
        
        return calibration_data
        
    except Exception as e:
        print(f"  âœ— Calibration data generation failed: {e}")
        traceback.print_exc()
        return None

def test_data_save_load():
    """æµ‹è¯•æ•°æ®ä¿å­˜å’ŒåŠ è½½"""
    print("\nğŸ” Testing data save/load...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = {
            "prompts": ["test prompt 1", "test prompt 2"],
            "xs": torch.randn(2, 2, 16, 32, 32),
            "ts": torch.tensor([[0.5, 0.0], [0.5, 0.0]]),
            "txt_embs": torch.randn(2, 2, 77, 4096),
        }
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_path = "/tmp/test_flux_calib.pt"
        torch.save(test_data, test_path)
        print("  âœ“ Test data saved")
        
        # åŠ è½½å¹¶éªŒè¯
        loaded_data = torch.load(test_path, map_location="cpu")
        
        for key in test_data.keys():
            if isinstance(test_data[key], torch.Tensor):
                assert torch.equal(test_data[key], loaded_data[key]), f"Tensor mismatch for {key}"
            else:
                assert test_data[key] == loaded_data[key], f"Value mismatch for {key}"
        
        print("  âœ“ Data save/load validation passed!")
        
        # æ¸…ç†
        os.remove(test_path)
        
        return True
        
    except Exception as e:
        print(f"  âœ— Data save/load failed: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Starting Flux.1 calibration data generation tests...\n")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA not available. This test requires GPU.")
        return
    
    print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\n")
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    success_count = 0
    total_tests = 5
    
    # 1. æµ‹è¯•æ¨¡å‹åŠ è½½
    model, t5, clip, ae = test_model_loading()
    if model is not None:
        success_count += 1
    
    # 2. æµ‹è¯•COCOæ•°æ®åŠ è½½
    prompts = test_coco_data_loading()
    if prompts is not None:
        success_count += 1
    
    # 3. æµ‹è¯•Fluxæ¨ç†
    if test_flux_inference():
        success_count += 1
    
    # 4. æµ‹è¯•æ ¡å‡†æ•°æ®ç”Ÿæˆ
    if test_calibration_data_generation():
        success_count += 1
    
    # 5. æµ‹è¯•æ•°æ®ä¿å­˜åŠ è½½
    if test_data_save_load():
        success_count += 1
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š Test Results: {success_count}/{total_tests} tests passed")
    
    if success_count == total_tests:
        print("ğŸ‰ All tests passed! The calibration data generation system is working correctly.")
        print("\nğŸ“ Next steps:")
        print("  1. Run the full calibration data generation:")
        print("     python scripts/gen_flux_calib_data.py --output_path flux_calib_data.pt")
        print("  2. Verify the generated data size and quality")
        print("  3. Proceed with quantization implementation")
    else:
        print("âŒ Some tests failed. Please check the error messages above.")
        print("   Common issues:")
        print("   - Model weights not downloaded (run with hf_download=True)")
        print("   - Insufficient GPU memory (try smaller batch size)")
        print("   - Missing dependencies (check requirements)")

if __name__ == "__main__":
    main() 
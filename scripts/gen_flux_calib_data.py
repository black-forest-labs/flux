import argparse
import torch
import os
import json
import math
from pathlib import Path
from omegaconf import OmegaConf
from pytorch_lightning import seed_everything
from typing import List, Dict, Any
import numpy as np
from tqdm import tqdm
import gc

import sys
# æ·»åŠ fluxæ¨¡å—åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from flux.util import load_flow_model, load_t5, load_clip, load_ae, configs
from flux.sampling import get_noise, prepare, get_schedule, denoise, unpack
from flux.model import Flux


def prepare_coco_text_and_image(json_file: str, max_samples: int = None) -> tuple[List[str], List[str]]:
    """
    å‡†å¤‡COCOæ•°æ®é›†çš„æ–‡æœ¬å’Œå›¾åƒè·¯å¾„
    
    Args:
        json_file: COCOæ ‡æ³¨æ–‡ä»¶è·¯å¾„
        max_samples: æœ€å¤§æ ·æœ¬æ•°é‡
    
    Returns:
        (captions, image_paths): æ–‡æœ¬æè¿°åˆ—è¡¨å’Œå›¾åƒè·¯å¾„åˆ—è¡¨
    """
    with open(json_file, 'r') as f:
        info = json.load(f)
    
    annotation_list = info["annotations"]
    image_caption_dict = {}
    
    for annotation_dict in annotation_list:
        image_id = annotation_dict["image_id"]
        if image_id in image_caption_dict:
            image_caption_dict[image_id].append(annotation_dict["caption"])
        else:
            image_caption_dict[image_id] = [annotation_dict["caption"]]
    
    captions = []
    image_paths = []
    
    # è·å–ç¬¬ä¸€ä¸ªcaptionå’Œå¯¹åº”çš„å›¾åƒè·¯å¾„
    for image_id, texts in image_caption_dict.items():
        captions.append(texts[0])
        # å‡è®¾å›¾åƒå­˜å‚¨åœ¨val2014ç›®å½•ä¸‹
        image_paths.append(f"COCO_val2014_{image_id:012}.jpg")
        
        if max_samples and len(captions) >= max_samples:
            break
    
    return captions, image_paths


def save_intermediate_states_with_offload(
    model: Flux,
    img: torch.Tensor,
    img_ids: torch.Tensor,
    txt: torch.Tensor,
    txt_ids: torch.Tensor,
    vec: torch.Tensor,
    timesteps: List[float],
    guidance: float = 0.0,
    offload: bool = True,
    device: str = "cuda"
) -> Dict[str, torch.Tensor]:
    """
    ä¿å­˜Fluxæ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´çŠ¶æ€ï¼Œä½¿ç”¨offloadç­–ç•¥ç®¡ç†å†…å­˜
    
    Args:
        model: Fluxæ¨¡å‹
        img: å›¾åƒæ½œåœ¨è¡¨ç¤º
        img_ids: å›¾åƒä½ç½®ç¼–ç 
        txt: æ–‡æœ¬åµŒå…¥
        txt_ids: æ–‡æœ¬ä½ç½®ç¼–ç 
        vec: CLIPå‘é‡åµŒå…¥
        timesteps: æ—¶é—´æ­¥åˆ—è¡¨
        guidance: å¼•å¯¼å¼ºåº¦
        offload: æ˜¯å¦ä½¿ç”¨offloadç­–ç•¥
        device: è®¾å¤‡
    
    Returns:
        åŒ…å«ä¸­é—´çŠ¶æ€çš„å­—å…¸
    """
    trajectory = {}
    outputs = {}
    torch_device = torch.device(device)
    
    # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
    model.eval()
    
    with torch.no_grad():
        # ä¿å­˜åˆå§‹å™ªå£°
        trajectory[timesteps[0]] = img.clone().cpu()  # ä¿å­˜åˆ°CPUä»¥èŠ‚çœGPUå†…å­˜
        
        # å¦‚æœä½¿ç”¨offloadï¼Œå°†æ¨¡å‹ç§»åˆ°GPU
        if offload:
            model = model.to(torch_device)
            torch.cuda.empty_cache()
        
        # é€æ­¥å»å™ª
        for i, (t_curr, t_prev) in enumerate(zip(timesteps[:-1], timesteps[1:])):
            t_vec = torch.full((img.shape[0],), t_curr, dtype=img.dtype, device=img.device)
            
            # é¢„æµ‹å™ªå£°
            pred = model(
                img=img,
                img_ids=img_ids,
                txt=txt,
                txt_ids=txt_ids,
                timesteps=t_vec,
                y=vec,
                guidance=torch.full_like(t_vec, guidance) if model.params.guidance_embed else None,
            )
            
            # æ›´æ–°å›¾åƒï¼ˆç®€åŒ–çš„æ¬§æ‹‰ç§¯åˆ†ï¼‰
            dt = t_prev - t_curr
            img = img + pred * dt
            
            # ä¿å­˜ä¸­é—´çŠ¶æ€åˆ°CPUä»¥èŠ‚çœGPUå†…å­˜
            trajectory[t_prev] = img.clone().cpu()
            outputs[t_prev] = pred.clone().cpu()
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # å¦‚æœä½¿ç”¨offloadï¼Œå°†æ¨¡å‹ç§»å›CPU
        if offload:
            model = model.cpu()
            torch.cuda.empty_cache()
    
    return {
        'trajectory': trajectory,
        'outputs': outputs
    }


def generate_flux_calibration_data_with_offload(
    prompts: List[str],
    model_name: str = "flux-dev",
    num_steps: int = 4,
    guidance: float = 0.0,
    height: int = 1024,
    width: int = 1024,
    seed: int = 42,
    device: str = "cuda",
    offload: bool = True
) -> Dict[str, Any]:
    """
    ä¸ºFluxæ¨¡å‹ç”Ÿæˆæ ¡å‡†æ•°æ®ï¼Œä½¿ç”¨offloadç­–ç•¥ç®¡ç†å¤§æ¨¡å‹å†…å­˜
    
    Args:
        prompts: æ–‡æœ¬æç¤ºåˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        num_steps: æ¨ç†æ­¥æ•°
        guidance: å¼•å¯¼å¼ºåº¦
        height: å›¾åƒé«˜åº¦
        width: å›¾åƒå®½åº¦
        seed: éšæœºç§å­
        device: è®¾å¤‡
        offload: æ˜¯å¦ä½¿ç”¨offload
    
    Returns:
        æ ¡å‡†æ•°æ®å­—å…¸
    """
    torch_device = torch.device(device)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    print(f"ğŸ”„ Loading Flux.1 model components with offload={'enabled' if offload else 'disabled'}...")
    
    # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ - æ¨¡ä»¿cli.pyçš„ç­–ç•¥
    t5 = load_t5(torch_device, max_length=256 if model_name == "flux-schnell" else 512)
    clip = load_clip(torch_device)
    model = load_flow_model(model_name, device="cpu" if offload else torch_device)
    ae = load_ae(model_name, device="cpu" if offload else torch_device)
    
    print(f"âœ“ Model components loaded")
    print(f"  T5: {sum(p.numel() for p in t5.parameters()) / 1e9:.2f}B params")
    print(f"  CLIP: {sum(p.numel() for p in clip.parameters()) / 1e9:.2f}B params") 
    print(f"  Flux: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B params")
    print(f"  VAE: {sum(p.numel() for p in ae.parameters()) / 1e9:.2f}B params")
    
    # è·å–è°ƒåº¦å™¨
    img_seq_len = (height // 16) * (width // 16) 
    timesteps = get_schedule(num_steps, img_seq_len, shift=(model_name != "flux-schnell"))
    
    all_trajectories = {}
    all_outputs = {}
    all_txt_embeddings = []
    all_vec_embeddings = []
    all_img_ids = []
    all_txt_ids = []
    
    print(f"ğŸ¯ Generating calibration data for {len(prompts)} prompts...")
    
    for i, prompt in enumerate(tqdm(prompts, desc="Processing prompts")):
        print(f"\nğŸ“ Processing prompt {i+1}/{len(prompts)}: {prompt[:50]}...")
        
        # å‡†å¤‡è¾“å…¥ - æ¨¡ä»¿cli.pyçš„é¡ºåº
        x = get_noise(
            1,
            height,
            width,
            device=torch_device,
            dtype=torch.bfloat16,
            seed=seed + i
        )
        
        # Offloadç­–ç•¥ï¼šå…ˆå°†VAEç§»åˆ°CPUï¼ŒT5å’ŒCLIPç§»åˆ°GPU
        if offload:
            ae = ae.cpu()
            torch.cuda.empty_cache()
            t5, clip = t5.to(torch_device), clip.to(torch_device)
        
        # å‡†å¤‡è¾“å…¥åµŒå…¥
        inp = prepare(t5, clip, x, prompt=prompt)
        
        # ä¿å­˜åµŒå…¥åˆ°CPU
        all_txt_embeddings.append(inp["txt"].cpu())
        all_vec_embeddings.append(inp["vec"].cpu()) 
        all_img_ids.append(inp["img_ids"].cpu())
        all_txt_ids.append(inp["txt_ids"].cpu())
        
        # Offloadç­–ç•¥ï¼šå°†T5å’ŒCLIPç§»åˆ°CPUï¼Œæ¨¡å‹ç§»åˆ°GPU
        if offload:
            t5, clip = t5.cpu(), clip.cpu()
            torch.cuda.empty_cache()
        
        # ç”Ÿæˆä¸­é—´çŠ¶æ€
        states = save_intermediate_states_with_offload(
            model=model,
            img=inp["img"],
            img_ids=inp["img_ids"],
            txt=inp["txt"],
            txt_ids=inp["txt_ids"], 
            vec=inp["vec"],
            timesteps=timesteps,
            guidance=guidance,
            offload=offload,
            device=device
        )
        
        # ä¿å­˜è½¨è¿¹æ•°æ®ï¼ˆå·²ç»åœ¨CPUä¸Šï¼‰
        for t, img_state in states['trajectory'].items():
            if t not in all_trajectories:
                all_trajectories[t] = []
            all_trajectories[t].append(img_state)
        
        # ä¿å­˜è¾“å‡ºæ•°æ®ï¼ˆå·²ç»åœ¨CPUä¸Šï¼‰
        for t, output_state in states['outputs'].items():
            if t not in all_outputs:
                all_outputs[t] = []
            all_outputs[t].append(output_state)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"âœ“ Processed prompt {i+1}, GPU memory: {torch.cuda.memory_allocated()/1e9:.2f}GB" if torch.cuda.is_available() else "âœ“ Processed prompt")
    
    print("\nğŸ”§ Assembling calibration data...")
    
    # æ•´ç†æ•°æ®æ ¼å¼
    timesteps_tensor = torch.tensor(timesteps[1:])  # æ’é™¤åˆå§‹æ—¶é—´æ­¥
    
    # æ‹¼æ¥è½¨è¿¹æ•°æ® [num_timesteps, batch_size, ...]
    xs_list = []
    outputs_list = []
    
    for t in timesteps[1:]:  # æ’é™¤åˆå§‹å™ªå£°
        if t in all_trajectories:
            xs_list.append(torch.stack(all_trajectories[t], dim=0))
        if t in all_outputs:
            outputs_list.append(torch.stack(all_outputs[t], dim=0))
    
    # è½¬æ¢ä¸º [batch_size, num_timesteps, ...]
    xs = torch.stack(xs_list, dim=1)  # [batch_size, num_timesteps, ...]
    outputs = torch.stack(outputs_list, dim=1)
    
    # æ‹¼æ¥åµŒå…¥æ•°æ®
    txt_embs = torch.stack(all_txt_embeddings, dim=0)  # [batch_size, seq_len, embed_dim]
    vec_embs = torch.stack(all_vec_embeddings, dim=0)  # [batch_size, embed_dim]
    img_ids = torch.stack(all_img_ids, dim=0)  # [batch_size, seq_len, 3]
    txt_ids = torch.stack(all_txt_ids, dim=0)  # [batch_size, seq_len, 3]
    
    # æ‰©å±•æ—¶é—´æ­¥å’ŒåµŒå…¥åˆ°æ‰€æœ‰æ—¶é—´æ­¥
    ts = timesteps_tensor.unsqueeze(1).repeat(1, len(prompts))  # [num_timesteps, batch_size]
    
    # ä¸ºæ¯ä¸ªæ—¶é—´æ­¥å¤åˆ¶åµŒå…¥
    num_timesteps = len(timesteps) - 1
    
    # æ£€æŸ¥å¹¶ä¿®æ­£å¼ é‡ç»´åº¦
    print(f"  ğŸ“Š Tensor shapes before expansion:")
    print(f"    txt_embs: {txt_embs.shape}")
    print(f"    vec_embs: {vec_embs.shape}")
    print(f"    img_ids: {img_ids.shape}")
    print(f"    txt_ids: {txt_ids.shape}")
    print(f"    num_timesteps: {num_timesteps}")
    
    # ä¸ºæ¯ä¸ªæ—¶é—´æ­¥å¤åˆ¶åµŒå…¥ï¼Œç¡®ä¿ç»´åº¦æ­£ç¡®
    if len(txt_embs.shape) == 3:  # [batch_size, seq_len, embed_dim]
        txt_embs_expanded = txt_embs.unsqueeze(0).repeat(num_timesteps, 1, 1, 1)  # [num_timesteps, batch_size, seq_len, embed_dim]
    else:
        # å¦‚æœå·²ç»æ˜¯4ç»´ï¼Œç›´æ¥å¤„ç†
        txt_embs_expanded = txt_embs
    
    if len(vec_embs.shape) == 2:  # [batch_size, embed_dim]
        vec_embs_expanded = vec_embs.unsqueeze(0).repeat(num_timesteps, 1, 1)  # [num_timesteps, batch_size, embed_dim]
    else:
        vec_embs_expanded = vec_embs
    
    if len(img_ids.shape) == 3:  # [batch_size, seq_len, 3]
        img_ids_expanded = img_ids.unsqueeze(0).repeat(num_timesteps, 1, 1, 1)  # [num_timesteps, batch_size, seq_len, 3]
    else:
        img_ids_expanded = img_ids
    
    if len(txt_ids.shape) == 3:  # [batch_size, seq_len, 3]
        txt_ids_expanded = txt_ids.unsqueeze(0).repeat(num_timesteps, 1, 1, 1)  # [num_timesteps, batch_size, seq_len, 3]
    else:
        txt_ids_expanded = txt_ids
    
    calibration_data = {
        "prompts": prompts,
        "ts": ts,  # [num_timesteps, batch_size]
        "xs": xs.transpose(0, 1),  # [num_timesteps, batch_size, ...]
        "outputs": outputs.transpose(0, 1),  # [num_timesteps, batch_size, ...]
        "txt_embs": txt_embs_expanded,  # [num_timesteps, batch_size, seq_len, embed_dim]
        "vec_embs": vec_embs_expanded,  # [num_timesteps, batch_size, embed_dim]
        "img_ids": img_ids_expanded,  # [num_timesteps, batch_size, seq_len, 3]
        "txt_ids": txt_ids_expanded,  # [num_timesteps, batch_size, seq_len, 3]
        "timesteps": timesteps,
        "guidance": guidance,
        "height": height,
        "width": width,
        "num_steps": num_steps,
        "model_name": model_name
    }
    
    print("âœ“ Calibration data assembled successfully!")
    
    return calibration_data


def main():
    parser = argparse.ArgumentParser(description="Generate calibration data for Flux.1 dev model")
    
    parser.add_argument(
        "--model_name",
        type=str,
        default="flux-dev",
        help="Model name (flux-dev or flux-schnell)"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        required=True,
        help="Path to save calibration data"
    )
    parser.add_argument(
        "--coco_annotations",
        type=str,
        default="/home/zcx/codes/data/coco/annotations/captions_val2014.json",
        help="Path to COCO annotations file"
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=32,
        help="Number of calibration samples"
    )
    parser.add_argument(
        "--num_steps",
        type=int,
        default=4,
        help="Number of denoising steps"
    )
    parser.add_argument(
        "--guidance",
        type=float,
        default=0.0,
        help="Guidance scale"
    )
    parser.add_argument(
        "--height",
        type=int,
        default=512,  # é»˜è®¤è¾ƒå°å°ºå¯¸å‡å°‘å†…å­˜ä½¿ç”¨
        help="Image height"
    )
    parser.add_argument(
        "--width",
        type=int,
        default=512,  # é»˜è®¤è¾ƒå°å°ºå¯¸å‡å°‘å†…å­˜ä½¿ç”¨
        help="Image width"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to use"
    )
    parser.add_argument(
        "--no_offload",
        action="store_true",
        help="Disable model offloading (requires more GPU memory)"
    )
    parser.add_argument(
        "--save_debug_images",
        type=str,
        default=None,
        help="Directory to save debug images"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹é…ç½®
    if args.model_name not in configs:
        available = ", ".join(configs.keys())
        raise ValueError(f"Got unknown model name: {args.model_name}, chose from {available}")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸ä¸º16çš„å€æ•°
    args.height = 16 * (args.height // 16)
    args.width = 16 * (args.width // 16)
    
    offload = not args.no_offload
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(args.seed)
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    print("ğŸš€ Starting Flux.1 calibration data generation...")
    print(f"ğŸ“‹ Configuration:")
    print(f"  Model: {args.model_name}")
    print(f"  Resolution: {args.width}x{args.height}")
    print(f"  Samples: {args.n_samples}")
    print(f"  Steps: {args.num_steps}")
    print(f"  Offload: {'enabled' if offload else 'disabled'}")
    print(f"  Device: {args.device}")
    
    if torch.cuda.is_available():
        print(f"  GPU: {torch.cuda.get_device_name()}")
        print(f"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # å‡†å¤‡COCOæ•°æ®
    print("\nğŸ“š Preparing COCO calibration data...")
    try:
        prompts, image_paths = prepare_coco_text_and_image(
            args.coco_annotations, 
            max_samples=args.n_samples
        )
        print(f"âœ“ Loaded {len(prompts)} prompts from COCO dataset")
        
        # æ‰“å°ä¸€äº›ç¤ºä¾‹æç¤º
        print("ğŸ“ Sample prompts:")
        for i, prompt in enumerate(prompts[:3]):
            print(f"  {i+1}: {prompt}")
        
    except Exception as e:
        print(f"âœ— Error loading COCO data: {e}")
        return
    
    # ç”Ÿæˆæ ¡å‡†æ•°æ®
    print("\nğŸ¯ Generating calibration data...")
    try:
        calibration_data = generate_flux_calibration_data_with_offload(
            prompts=prompts,
            model_name=args.model_name,
            num_steps=args.num_steps,
            guidance=args.guidance,
            height=args.height,
            width=args.width,
            seed=args.seed,
            device=args.device,
            offload=offload
        )
        
        print("\nâœ… Successfully generated calibration data")
        print(f"ğŸ“Š Data summary:")
        for key, value in calibration_data.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape} ({value.dtype})")
            elif isinstance(value, list) and key == "prompts":
                print(f"  {key}: {len(value)} prompts")
            else:
                print(f"  {key}: {value}")
                
    except Exception as e:
        print(f"âœ— Error generating calibration data: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ä¿å­˜æ ¡å‡†æ•°æ®
    print(f"\nğŸ’¾ Saving calibration data to {args.output_path}...")
    try:
        torch.save(calibration_data, args.output_path)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(args.output_path) / 1e9
        print(f"âœ“ Successfully saved calibration data ({file_size:.2f}GB)")
        
        # éªŒè¯ä¿å­˜çš„æ•°æ®
        loaded_data = torch.load(args.output_path, map_location='cpu')
        print(f"âœ“ Verification: loaded data contains {len(loaded_data)} keys")
        
    except Exception as e:
        print(f"âœ— Error saving calibration data: {e}")
        return
    
    # å¯é€‰ï¼šä¿å­˜è°ƒè¯•å›¾åƒ
    if args.save_debug_images:
        print(f"\nğŸ–¼ï¸ Saving debug images to {args.save_debug_images}...")
        os.makedirs(args.save_debug_images, exist_ok=True)
        
        try:
            # é‡æ–°åŠ è½½VAEè¿›è¡Œè§£ç 
            ae = load_ae(args.model_name, device=args.device)
            
            # ç”Ÿæˆå‡ å¼ ç¤ºä¾‹å›¾åƒ
            with torch.no_grad():
                for i, prompt in enumerate(prompts[:3]):
                    # è·å–æœ€ç»ˆå»å™ªç»“æœ
                    final_latent = calibration_data["xs"][-1, i:i+1].to(args.device)  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    
                    # è§£ç ä¸ºå›¾åƒ
                    final_latent = unpack(final_latent, args.height, args.width)
                    with torch.autocast(device_type=args.device, dtype=torch.bfloat16):
                        decoded = ae.decode(final_latent)
                    
                    # è½¬æ¢å¹¶ä¿å­˜
                    from PIL import Image
                    decoded = torch.clamp(decoded, -1, 1)
                    decoded = (decoded + 1) * 127.5
                    decoded = decoded.cpu().numpy().astype(np.uint8)
                    decoded = decoded[0].transpose(1, 2, 0)  # CHW -> HWC
                    
                    img = Image.fromarray(decoded)
                    img.save(os.path.join(args.save_debug_images, f"debug_{i:03d}.png"))
                    
            print(f"âœ“ Saved {min(3, len(prompts))} debug images")
            
        except Exception as e:
            print(f"âš ï¸ Warning: Could not save debug images: {e}")
    
    print("\nğŸ‰ Calibration data generation completed successfully!")
    print(f"ğŸ“ˆ Memory usage peak: {torch.cuda.max_memory_allocated()/1e9:.2f}GB" if torch.cuda.is_available() else "")
    print("\nğŸ“ Next steps:")
    print("  1. Verify the calibration data quality")
    print("  2. Implement PTQ (Post-Training Quantization) for Flux.1")
    print("  3. Run sensitivity analysis")
    print("  4. Generate mixed precision configuration")


if __name__ == "__main__":
    main() 
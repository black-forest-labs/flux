#!/usr/bin/env python3
"""
å¸¦offloadçš„æ ¡å‡†æ•°æ®ç”Ÿæˆæµ‹è¯•è„šæœ¬
ä¸“é—¨æµ‹è¯•Flux.1 devå¤§æ¨¡å‹çš„å†…å­˜ç®¡ç†
"""

import os
import sys
import torch
import traceback
from pathlib import Path

# æ·»åŠ fluxæ¨¡å—åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜çŠ¶æ€"""
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        print(f"  ğŸ“Š GPU Memory: {allocated:.2f}GB allocated, {cached:.2f}GB cached, {total_memory:.2f}GB total")
        return total_memory, allocated, cached
    return 0, 0, 0

def test_model_loading_with_offload():
    """æµ‹è¯•å¸¦offloadçš„æ¨¡å‹åŠ è½½"""
    print("ğŸ” Testing model loading with offload...")
    
    try:
        from flux.util import load_flow_model, load_t5, load_clip, load_ae
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_device = torch.device(device)
        
        print("  ğŸ’¾ Initial memory state:")
        check_gpu_memory()
        
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
        local_model_path = "/home/zcx/codes/flux1dev"
        if os.path.exists(local_model_path):
            print(f"  ğŸ“ Using local models from: {local_model_path}")
        else:
            print("  ğŸŒ Will download models from HuggingFace")
        
        # æ¨¡ä»¿cli.pyçš„offloadç­–ç•¥
        print("  ğŸ“¥ Loading T5 to GPU...")
        t5 = load_t5(torch_device, max_length=512)
        check_gpu_memory()
        
        print("  ğŸ“¥ Loading CLIP to GPU...")
        clip = load_clip(torch_device)
        check_gpu_memory()
        
        print("  ğŸ“¥ Loading Flux model to CPU...")
        # å¦‚æœæœ‰æœ¬åœ°æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°è·¯å¾„
        if os.path.exists(os.path.join(local_model_path, "flux1-dev.safetensors")):
            print("  ğŸ“ Loading from local safetensors file...")
            # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
            model = load_flow_model("flux-dev", device="cpu", hf_download=False)
        else:
            print("  ğŸŒ Downloading from HuggingFace...")
            model = load_flow_model("flux-dev", device="cpu")  # å…ˆåŠ è½½åˆ°CPU
        
        print(f"    Flux parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
        check_gpu_memory()
        
        print("  ğŸ“¥ Loading VAE to CPU...")
        ae = load_ae("flux-dev", device="cpu")  # å…ˆåŠ è½½åˆ°CPU
        print(f"    VAE parameters: {sum(p.numel() for p in ae.parameters()) / 1e9:.2f}B")
        check_gpu_memory()
        
        # æµ‹è¯•offloadï¼šå°†T5å’ŒCLIPç§»åˆ°CPUï¼Œæ¨¡å‹ç§»åˆ°GPU
        print("  ğŸ”„ Testing offload: T5 & CLIP to CPU, Model to GPU...")
        t5, clip = t5.cpu(), clip.cpu()
        torch.cuda.empty_cache()
        model = model.to(torch_device)
        check_gpu_memory()
        
        # æµ‹è¯•offloadï¼šæ¨¡å‹ç§»å›CPUï¼ŒVAEç§»åˆ°GPU
        print("  ğŸ”„ Testing offload: Model to CPU, VAE to GPU...")
        model = model.cpu()
        torch.cuda.empty_cache()
        ae.decoder.to(torch_device)  # åªå°†decoderç§»åˆ°GPU
        check_gpu_memory()
        
        print("  âœ… Offload test passed!")
        return True
        
    except Exception as e:
        print(f"  âŒ Offload test failed: {e}")
        traceback.print_exc()
        return False

def test_simple_model_loading():
    """æµ‹è¯•ç®€åŒ–çš„æ¨¡å‹åŠ è½½ï¼ˆä»…åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼‰"""
    print("ğŸ” Testing simple model loading (text encoders only)...")
    
    try:
        from flux.util import load_t5, load_clip
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_device = torch.device(device)
        
        print("  ğŸ’¾ Initial memory state:")
        check_gpu_memory()
        
        print("  ğŸ“¥ Loading T5...")
        t5 = load_t5(torch_device, max_length=512)
        check_gpu_memory()
        
        print("  ğŸ“¥ Loading CLIP...")
        clip = load_clip(torch_device)
        check_gpu_memory()
        
        # æµ‹è¯•offload
        print("  ğŸ”„ Testing T5 & CLIP offload...")
        t5, clip = t5.cpu(), clip.cpu()
        torch.cuda.empty_cache()
        check_gpu_memory()
        
        print("  âœ… Simple model loading test passed!")
        return True
        
    except Exception as e:
        print(f"  âŒ Simple model loading failed: {e}")
        traceback.print_exc()
        return False

def test_minimal_calibration():
    """æµ‹è¯•æœ€å°çš„æ ¡å‡†æ•°æ®ç”Ÿæˆ"""
    print("\nğŸ§ª Testing minimal calibration data generation...")
    
    try:
        sys.path.append(os.path.dirname(__file__))
        from gen_flux_calib_data import generate_flux_calibration_data_with_offload, prepare_coco_text_and_image
        
        # å‡†å¤‡æœ€å°æ•°æ®é›†
        coco_path = "/home/zcx/codes/data/coco/annotations/captions_val2014.json"
        if not os.path.exists(coco_path):
            print(f"  âš ï¸ COCO file not found: {coco_path}")
            print("  ğŸ“ Using synthetic prompts instead...")
            prompts = ["A beautiful landscape", "A cat sitting on a table"]
        else:
            prompts, _ = prepare_coco_text_and_image(coco_path, max_samples=2)
        
        print(f"  ğŸ“ Using {len(prompts)} prompts:")
        for i, prompt in enumerate(prompts):
            print(f"    {i+1}: {prompt}")
        
        print("  ğŸš€ Starting calibration with offload...")
        initial_memory = check_gpu_memory()
        
        # ç”Ÿæˆæ ¡å‡†æ•°æ®ï¼ˆä½¿ç”¨æœ€å°é…ç½®ï¼‰
        calibration_data = generate_flux_calibration_data_with_offload(
            prompts=prompts,
            model_name="flux-dev",
            num_steps=2,      # æœ€å°‘æ­¥æ•°
            guidance=0.0,
            height=256,       # æœ€å°å°ºå¯¸
            width=256,
            seed=42,
            device="cuda" if torch.cuda.is_available() else "cpu",
            offload=True
        )
        
        final_memory = check_gpu_memory()
        
        print("  âœ… Minimal calibration completed!")
        print(f"  ğŸ“Š Data generated:")
        for key, value in calibration_data.items():
            if isinstance(value, torch.Tensor):
                print(f"    {key}: {value.shape}")
            elif key == "prompts":
                print(f"    {key}: {len(value)} items")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Minimal calibration failed: {e}")
        traceback.print_exc()
        return False

def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡"""
    print("\nğŸ’¾ Testing memory efficiency...")
    
    if not torch.cuda.is_available():
        print("  âš ï¸ CUDA not available, skipping memory test")
        return True
    
    try:
        # è®°å½•åˆå§‹å†…å­˜
        torch.cuda.empty_cache()
        initial_allocated = torch.cuda.memory_allocated()
        initial_cached = torch.cuda.memory_reserved()
        
        print(f"  ğŸ“Š Initial memory: {initial_allocated/1e9:.2f}GB allocated, {initial_cached/1e9:.2f}GB cached")
        
        # è¿è¡Œç®€åŒ–æµ‹è¯•è€Œä¸æ˜¯å®Œæ•´æ ¡å‡†
        success = test_simple_model_loading()
        
        # å¼ºåˆ¶æ¸…ç†
        torch.cuda.empty_cache()
        final_allocated = torch.cuda.memory_allocated()
        final_cached = torch.cuda.memory_reserved()
        
        print(f"  ğŸ“Š Final memory: {final_allocated/1e9:.2f}GB allocated, {final_cached/1e9:.2f}GB cached")
        print(f"  ğŸ“ˆ Memory delta: {(final_allocated-initial_allocated)/1e9:.2f}GB allocated, {(final_cached-initial_cached)/1e9:.2f}GB cached")
        
        if final_allocated - initial_allocated < 1e9:  # å°äº1GBå¢é•¿
            print("  âœ… Memory efficiency test passed!")
            return True
        else:
            print("  âš ï¸ Memory usage higher than expected")
            return False
            
    except Exception as e:
        print(f"  âŒ Memory efficiency test failed: {e}")
        traceback.print_exc()
        return False

def test_cpu_fallback():
    """æµ‹è¯•CPUå›é€€æœºåˆ¶"""
    print("\nğŸ–¥ï¸ Testing CPU fallback...")
    
    try:
        sys.path.append(os.path.dirname(__file__))
        
        # åªæµ‹è¯•åŸºæœ¬å¯¼å…¥ï¼Œä¸è¿›è¡Œå®é™…æ¨ç†
        from gen_flux_calib_data import prepare_coco_text_and_image
        
        coco_path = "/home/zcx/codes/data/coco/annotations/captions_val2014.json"
        if os.path.exists(coco_path):
            prompts, _ = prepare_coco_text_and_image(coco_path, max_samples=1)
            print(f"  âœ… COCO data loading works: {prompts[0][:50]}...")
        else:
            print("  âš ï¸ COCO data not found, but function works")
        
        print("  âœ… CPU fallback test passed!")
        return True
        
    except Exception as e:
        print(f"  âŒ CPU fallback failed: {e}")
        print("  â„¹ï¸ This is expected for large models on limited RAM")
        return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Starting Flux.1 calibration with offload tests...\n")
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    if torch.cuda.is_available():
        print(f"ğŸ’» GPU: {torch.cuda.get_device_name()}")
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ’¾ GPU Memory: {total_memory:.1f}GB")
        
        if total_memory < 20:
            print("âš ï¸  GPU memory < 20GB. Offload is strongly recommended.")
        elif total_memory < 40:
            print("â„¹ï¸  GPU memory < 40GB. Offload is recommended.")
        else:
            print("âœ… GPU memory >= 40GB. Offload optional but still beneficial.")
    else:
        print("âŒ CUDA not available")
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹
    local_model_path = "/home/zcx/codes/flux1dev"
    if os.path.exists(local_model_path):
        print(f"ğŸ“ Local model directory found: {local_model_path}")
        # åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
        model_files = []
        for file in os.listdir(local_model_path):
            if file.endswith(('.safetensors', '.bin')):
                model_files.append(file)
        if model_files:
            print(f"ğŸ“„ Available model files: {', '.join(model_files)}")
    else:
        print("ğŸŒ No local models found, will use HuggingFace")
    
    print()
    
    # è¿è¡Œæµ‹è¯• - ä¿®æ”¹æµ‹è¯•é¡ºåºï¼Œå…ˆè¿è¡Œç®€å•æµ‹è¯•
    tests = [
        ("Simple Model Loading", test_simple_model_loading),
        ("Memory Efficiency", test_memory_efficiency), 
        ("CPU Fallback", test_cpu_fallback),
        # ("Full Model Loading with Offload", test_model_loading_with_offload),  # ç§»åˆ°æœ€åæˆ–è·³è¿‡
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"{'='*60}")
        print(f"ğŸ§ª {test_name}")
        print(f"{'='*60}")
        
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} PASSED\n")
            else:
                print(f"âŒ {test_name} FAILED\n")
        except Exception as e:
            print(f"ğŸ’¥ {test_name} CRASHED: {e}\n")
    
    # æ€»ç»“
    print(f"{'='*60}")
    print(f"ğŸ“Š Test Summary: {passed}/{total} tests passed")
    print(f"{'='*60}")
    
    if passed >= total - 1:  # å…è®¸ä¸€ä¸ªæµ‹è¯•å¤±è´¥
        print("ğŸ‰ Basic tests passed! The core functionality is working.")
        print("\nğŸ“ Recommendations for production:")
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            if total_memory < 24:
                print("  âš ï¸  Use smaller image sizes (256x256 or 512x512)")
                print("  âš ï¸  Reduce num_steps (2-4 steps)")
                print("  âš ï¸  Process samples one by one")
                print("  âœ… Keep offload=True")
            else:
                print("  âœ… You can use standard sizes (512x512 or 1024x1024)")
                print("  âœ… Keep offload=True for optimal memory usage")
        
        print("\nğŸš€ Ready to test basic calibration:")
        print("python scripts/gen_flux_calib_data.py --output_path test_calib.pt --n_samples 2 --height 256 --width 256 --num_steps 2")
        
    else:
        print("âŒ Most tests failed. Check the error messages above.")
        print("ğŸ”§ Common solutions:")
        print("  - Ensure you have enough GPU memory (>20GB recommended)")
        print("  - Try smaller image sizes")
        print("  - Check that all dependencies are installed")
        print("  - Verify model weights are accessible")

if __name__ == "__main__":
    main() 